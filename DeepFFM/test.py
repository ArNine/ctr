import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split


class ConvLayer(nn.Module):
    def __init__(self, c_in):
        super(ConvLayer, self).__init__()
        self.downConv = nn.Conv1d(in_channels=c_in,
                                  out_channels=c_in,
                                  kernel_size=3,
                                  padding=1,
                                  padding_mode='circular')
        self.norm = nn.BatchNorm1d(c_in)
        self.activation = nn.ELU()
        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)

    def forward(self, x):
        x = self.downConv(x.permute(0, 2, 1))
        x = self.norm(x)
        x = self.activation(x)
        x = self.maxPool(x)
        x = x.transpose(1,2)
        return x

import matplotlib.pyplot as plt
def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    val_metrics = dfhistory['val_' + metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation ' + metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_" + metric, 'val_' + metric])
    plt.show()
ss = """
0     1.0  0.435231  0.688109  0.407815  0.736899
1     2.0  0.416225  0.723278  0.405751  0.743248
2     3.0  0.411099  0.734380  0.405880  0.746758
3     4.0  0.407728  0.740985  0.401974  0.752463
4     5.0  0.405490  0.745299  0.399769  0.755343
5     6.0  0.403999  0.748298  0.398358  0.758738
6     7.0  0.401998  0.752346  0.398657  0.762879
7     8.0  0.398342  0.759761  0.393671  0.767990
8     9.0  0.394049  0.768130  0.391411  0.771344
9    10.0  0.389891  0.775453  0.390703  0.772858
10   11.0  0.385667  0.782579  0.389328  0.775096
11   12.0  0.381205  0.789969  0.388590  0.776402
12   13.0  0.376510  0.797158  0.388127  0.777268
13   14.0  0.371883  0.804216  0.388590  0.776559
14   15.0  0.368547  0.809042  0.388249  0.776688
15   16.0  0.366254  0.812151  0.389178  0.775594
16   17.0  0.365093  0.813778  0.388566  0.776451
17   18.0  0.364080  0.815274  0.389024  0.775860
18   19.0  0.363517  0.816078  0.389064  0.775836
19   20.0  0.362884  0.816834  0.389456  0.775746
20   21.0  0.362471  0.817461  0.389133  0.775468
21   22.0  0.362228  0.817826  0.389403  0.775141
22   23.0  0.361757  0.818480  0.390244  0.774905
23   24.0  0.361296  0.819112  0.389597  0.775298
24   25.0  0.360801  0.819732  0.389968  0.774762
25   26.0  0.360431  0.820157  0.389293  0.775243
26   27.0  0.360030  0.820700  0.389241  0.775428
27   28.0  0.359761  0.821208  0.389611  0.774994
28   29.0  0.359425  0.821710  0.389479  0.775596
29   30.0  0.359200  0.821886  0.389460  0.775085
30   31.0  0.359207  0.821859  0.389550  0.774684
31   32.0  0.359847  0.821028  0.389976  0.774247
32   33.0  0.359521  0.821389  0.389627  0.774821
33   34.0  0.359477  0.821407  0.390080  0.773947
34   35.0  0.359247  0.821724  0.389411  0.775079
35   36.0  0.359321  0.821660  0.389537  0.774487
36   37.0  0.359110  0.821972  0.389588  0.774726
37   38.0  0.358856  0.822308  0.390383  0.773383
38   39.0  0.358627  0.822650  0.390107  0.774171
39   40.0  0.358481  0.822801  0.391323  0.773273
40   41.0  0.358432  0.822798  0.391224  0.772202
41   42.0  0.358487  0.822784  0.390782  0.772757
42   43.0  0.358418  0.822929  0.391115  0.772284
43   44.0  0.358573  0.822756  0.390871  0.772175
44   45.0  0.358319  0.823022  0.391098  0.772177
45   46.0  0.358400  0.822976  0.392099  0.771547
46   47.0  0.358211  0.823130  0.391814  0.771056
47   48.0  0.358235  0.823159  0.391938  0.771120
48   49.0  0.358167  0.823241  0.392048  0.770721
49   50.0  0.358148  0.823372  0.392458  0.769796"""


s2 = """0     1.0  0.423052  0.707595  0.408490  0.737052
1     2.0  0.413576  0.728977  0.406940  0.741109
2     3.0  0.409498  0.737650  0.402479  0.748757
3     4.0  0.406161  0.744252  0.400593  0.752673
4     5.0  0.404328  0.747806  0.400477  0.753648
5     6.0  0.402334  0.751833  0.396615  0.759980
6     7.0  0.399426  0.757838  0.395318  0.764449
7     8.0  0.396041  0.764376  0.392795  0.768008
8     9.0  0.392269  0.771129  0.390482  0.772173
9    10.0  0.386703  0.781024  0.388682  0.775576
10   11.0  0.380391  0.791337  0.388367  0.777126
11   12.0  0.374378  0.800696  0.387898  0.776881
12   13.0  0.369237  0.808090  0.387916  0.776440
13   14.0  0.365834  0.813079  0.388101  0.776255
14   15.0  0.363319  0.816322  0.388206  0.775816
15   16.0  0.361828  0.818448  0.388296  0.775802
16   17.0  0.360491  0.820118  0.388423  0.776041
17   18.0  0.359149  0.821704  0.388268  0.775749
18   19.0  0.357951  0.823248  0.389296  0.774567
19   20.0  0.356554  0.825063  0.389318  0.774753
20   21.0  0.353952  0.828321  0.390448  0.772749
21   22.0  0.350026  0.833479  0.389624  0.772730
22   23.0  0.344521  0.840046  0.391651  0.769560
23   24.0  0.343977  0.839554  0.395062  0.768857
24   25.0  0.341526  0.841968  0.391481  0.772106
25   26.0  0.336656  0.846818  0.393012  0.770010
26   27.0  0.334271  0.849392  0.392304  0.770436
27   28.0  0.334409  0.848616  0.391081  0.772522
28   29.0  0.334150  0.848280  0.396617  0.767227
29   30.0  0.331818  0.850795  0.393618  0.768837
30   31.0  0.329684  0.853375  0.394487  0.768808
31   32.0  0.329073  0.854331  0.398226  0.765346
32   33.0  0.329696  0.853919  0.400498  0.763989
33   34.0  0.329454  0.854122  0.395902  0.768554
34   35.0  0.328062  0.855632  0.395465  0.768360
35   36.0  0.326604  0.857298  0.396154  0.767361
36   37.0  0.325946  0.858199  0.398130  0.767910
37   38.0  0.326384  0.857653  0.396353  0.766969
38   39.0  0.326964  0.857031  0.393163  0.770819
39   40.0  0.326598  0.857352  0.393402  0.770272
40   41.0  0.325801  0.858351  0.398915  0.766311
41   42.0  0.325379  0.858794  0.394743  0.768995
42   43.0  0.325494  0.858687  0.400118  0.766784
43   44.0  0.325932  0.858305  0.393538  0.771486
44   45.0  0.326218  0.857943  0.393436  0.771299
45   46.0  0.325872  0.858208  0.397603  0.767804
46   47.0  0.325234  0.858882  0.400068  0.766574
47   48.0  0.325582  0.858682  0.396088  0.767878
48   49.0  0.325838  0.858399  0.396837  0.768278
49   50.0  0.325951  0.858137  0.394088  0.770898"""


s3 = """0     1.0  0.426452  0.700665  0.407807  0.738203
1     2.0  0.413588  0.728489  0.405841  0.741708
2     3.0  0.408951  0.738912  0.402250  0.749686
3     4.0  0.405211  0.746270  0.404594  0.747893
4     5.0  0.403470  0.749630  0.400339  0.755740
5     6.0  0.401378  0.753732  0.397522  0.760591
6     7.0  0.398003  0.760471  0.397055  0.762808
7     8.0  0.393928  0.768400  0.393666  0.768745
8     9.0  0.388473  0.778116  0.392706  0.771919
9    10.0  0.382684  0.787791  0.389633  0.775551
10   11.0  0.376581  0.797581  0.392933  0.774683
11   12.0  0.370928  0.806039  0.387636  0.777288
12   13.0  0.366682  0.811908  0.388130  0.776415
13   14.0  0.363638  0.816012  0.388893  0.775229
14   15.0  0.361447  0.818968  0.388985  0.775845
15   16.0  0.359842  0.821024  0.389078  0.775021
16   17.0  0.358530  0.822672  0.389427  0.774668
17   18.0  0.357269  0.824265  0.389555  0.774324
18   19.0  0.355469  0.826621  0.391125  0.772314
19   20.0  0.353029  0.829645  0.390280  0.772704
20   21.0  0.349476  0.833985  0.391286  0.771329
21   22.0  0.344719  0.839779  0.393393  0.769768
22   23.0  0.341324  0.843201  0.396667  0.767330
23   24.0  0.340371  0.843558  0.394003  0.768980
24   25.0  0.338017  0.845820  0.396464  0.768203
25   26.0  0.336984  0.847025  0.398156  0.764740
26   27.0  0.337238  0.846578  0.395331  0.765758
27   28.0  0.338146  0.844878  0.393493  0.769315
28   29.0  0.337032  0.845814  0.399276  0.765433
29   30.0  0.334975  0.847745  0.398070  0.765236
30   31.0  0.333931  0.848495  0.397176  0.765780
31   32.0  0.333523  0.848561  0.396767  0.766795
32   33.0  0.332730  0.849732  0.394082  0.768518
33   34.0  0.331160  0.851857  0.394919  0.767707
34   35.0  0.329678  0.853678  0.398452  0.765168
35   36.0  0.328904  0.854230  0.397128  0.767273
36   37.0  0.328710  0.854340  0.394286  0.768955
37   38.0  0.328362  0.854761  0.394814  0.768996
38   39.0  0.328129  0.854912  0.395382  0.767678
39   40.0  0.327052  0.856343  0.398391  0.766712
40   41.0  0.326459  0.856950  0.396705  0.768806
41   42.0  0.326162  0.857230  0.394218  0.769541
42   43.0  0.326297  0.857268  0.392514  0.771195
43   44.0  0.326692  0.856773  0.399555  0.764592
44   45.0  0.327035  0.856380  0.396575  0.766456
45   46.0  0.327022  0.856392  0.393929  0.769715
46   47.0  0.327832  0.855438  0.396379  0.768377
47   48.0  0.328033  0.855452  0.397932  0.767225
48   49.0  0.328156  0.855400  0.397237  0.766731
49   50.0  0.328295  0.855329  0.396611  0.768597"""

import pandas as pd
dfhistory = pd.DataFrame(columns=['epoch', 'loss', 'auc', 'val_loss', 'val_auc'])
for i, index in enumerate(ss.split("\n")):
    val = index.replace('    ', ' ').replace('  ', ' ').split(' ')
    print(val)
    if i < 10:
        dfhistory.loc[i] = (int(val[0]), float(val[2]), float(val[3]), float(val[4]), float(val[5]))
    else:
        dfhistory.loc[i] = (int(val[0]), float(val[3]), float(val[4]), float(val[5]), float(val[6]))

plot_metric(dfhistory, 'loss')
plot_metric(dfhistory, 'auc')
print()

import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split



import matplotlib.pyplot as plt
def plot_metric(dfhistory, metric):
    SACSN = dfhistory["SACSN_" + metric]
    PNN = dfhistory['PNN_' + metric]
    BST = dfhistory['BST_' + metric]
    DeepFFMS = dfhistory["DeepFFMS_" + metric]
    DeepFM = dfhistory["DeepFM_" + metric]
    epochs = list(dfhistory['epoch'])
    # print(epochs)
    plt.plot(epochs, DeepFM, 'co--')
    plt.plot(epochs, BST, 'go--')
    plt.plot(epochs, SACSN, 'bo--')
    # plt.plot(epochs, PNN, 'ro--')

    plt.plot(epochs, DeepFFMS, 'yo-')

    # plt.title('model ' + metric)
    plt.xlabel("Epoch")
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.ylabel(metric)
    plt.legend(["第一组", "第二组", "第三组", "第四组"])
    plt.show()


ss = """0     1.0  0.527337  0.683341  0.413815  0.736899 0.407815  0.716899 0.550802  0.636995 0.388677  0.737565
1     2.0  0.517553  0.704740  0.405751  0.743248 0.405751  0.733248 0.518481  0.691539 0.386976  0.743238
2     3.0  0.510523  0.714906  0.405880  0.746758 0.405880  0.736758 0.509238  0.708887 0.382841  0.750556
3     4.0  0.507891  0.718689  0.401974  0.752463 0.401974  0.742463 0.502999  0.718802 0.381907  0.755724
4     5.0  0.505899  0.721963  0.399769  0.755343 0.399769  0.745343 0.499760  0.723744 0.379122  0.758457
5     6.0  0.505111  0.724300  0.398358  0.758738 0.398358  0.738738 0.497080  0.727796 0.376051  0.767240
6     7.0  0.503549  0.725278  0.398657  0.762879 0.398657  0.752879 0.495036  0.730867 0.370532  0.773986
7     8.0  0.503299  0.727129  0.393671  0.767990 0.393671  0.757990 0.493897  0.732978 0.365131  0.779244
8     9.0  0.501551  0.729379  0.391411  0.771344 0.391411  0.761344 0.492397  0.735266 0.366929  0.782939
9    10.0  0.500531  0.730664  0.390703  0.772858 0.390703  0.765858 0.490608  0.737503 0.376198  0.778897
10   11.0  0.499934  0.733307  0.389328  0.775096 0.389328  0.769096 0.489643  0.739422 0.376815  0.776857
11   12.0  0.498198  0.734672  0.388590  0.776402 0.388590  0.772402 0.488281  0.741732 0.380093  0.777247
12   13.0  0.497461  0.736180  0.388127  0.777268 0.388127  0.777268 0.487188  0.743029 0.381445  0.775244
13   14.0  0.496254  0.738370  0.388590  0.776559 0.388590  0.776559 0.485687  0.745285 0.382312  0.775018
14   15.0  0.495149  0.739825  0.388249  0.776688 0.388249  0.776688 0.484448  0.747363 0.384166  0.773297
15   16.0  0.493824  0.742938  0.389178  0.775594 0.389178  0.775594 0.482864  0.749657 0.384786  0.773286
16   17.0  0.491346  0.745918  0.388566  0.776451 0.388566  0.776451 0.480726  0.752586 0.385533  0.773379
17   18.0  0.489920  0.747995  0.389024  0.775860 0.389024  0.775860 0.478897  0.755311 0.387067  0.771077
18   19.0  0.488925  0.751642  0.389064  0.775836 0.389064  0.775836 0.476405  0.759045 0.392204  0.769163
19   20.0  0.485689  0.754424  0.389456  0.775746 0.389456  0.775746 0.474147  0.761840 0.391174  0.770368
20   21.0  0.484385  0.756118  0.389133  0.775468 0.389133  0.775468 0.475088  0.765171 0.390568  0.770156
21   22.0  0.482689  0.760080  0.389403  0.775141 0.389403  0.775141 0.479813  0.768001 0.390241  0.769542
22   23.0  0.482394  0.762118  0.390244  0.774905 0.390244  0.774905 0.477401  0.771210 0.390842  0.767113
23   24.0  0.479250  0.763800  0.389597  0.775298 0.389597  0.775298 0.475532  0.775961 0.391042  0.766559
24   25.0  0.477555  0.765845  0.389968  0.774762 0.389968  0.774762 0.473183  0.781736 0.390503  0.769053
25   26.0  0.479510  0.767081  0.389293  0.775243 0.389293  0.775243 0.470936  0.788633 0.390949  0.768689
26   27.0  0.476947  0.768737  0.389241  0.775428 0.389241  0.775428 0.477945  0.789353 0.391297  0.768519
27   28.0  0.474809  0.769855  0.389611  0.774994 0.389611  0.774994 0.475326  0.791678 0.391067  0.770260
28   29.0  0.474233  0.770364  0.389479  0.775596 0.389479  0.775596 0.472205  0.795779 0.390940  0.767531
29   30.0  0.473373  0.771510  0.389460  0.775085 0.389460  0.775085 0.470773  0.797502 0.390148  0.768786
30   31.0  0.474505  0.771499  0.389550  0.774684 0.389550  0.774684 0.470768  0.802204 0.389165  0.768383
31   32.0  0.477096  0.768597  0.389976  0.774247 0.389976  0.774247 0.468900  0.805665 0.388811  0.769350
32   33.0  0.498811  0.754537  0.389627  0.774821 0.389627  0.774821 0.468538  0.801965 0.387617  0.771777
33   34.0  0.513595  0.751853  0.390080  0.773947 0.390080  0.773947 0.466607  0.802461 0.386459  0.769209
34   35.0  0.525469  0.744945  0.389411  0.775079 0.389411  0.775079 0.465294  0.803596 0.385555  0.768643
35   36.0  0.538001  0.763065  0.389537  0.774487 0.389537  0.774487 0.464182  0.807389 0.384915  0.768760
36   37.0  0.538168  0.742289  0.389588  0.774726 0.389588  0.774726 0.464180  0.805953 0.383719  0.769243
37   38.0  0.561454  0.739834  0.390383  0.773383 0.390383  0.773383 0.463933  0.804334 0.383578  0.769952
38   39.0  0.554247  0.739229  0.390107  0.774171 0.390107  0.774171 0.464677  0.805903 0.383730  0.768562
39   40.0  0.569267  0.736310  0.391323  0.773273 0.391323  0.773273 0.465929  0.805699 0.383581  0.768936
40   41.0  0.580257  0.759159  0.391224  0.772202 0.391224  0.772202 0.466803  0.805903 0.383586  0.768744
41   42.0  0.582804  0.731053  0.390782  0.772757 0.390782  0.772757 0.465490  0.806567 0.383649  0.770778
42   43.0  0.598234  0.728541  0.391115  0.772284 0.391115  0.772284 0.464115  0.806681 0.383721  0.770792
43   44.0  0.592793  0.732210  0.390871  0.772175 0.390871  0.772175 0.464224  0.806746 0.383856  0.770102
44   45.0  0.599544  0.728582  0.391098  0.772177 0.391098  0.772177 0.464990  0.806898 0.383995  0.768239
45   46.0  0.627793  0.755540  0.392099  0.771547 0.392099  0.771547 0.464811  0.808949 0.384294  0.770366
46   47.0  0.622444  0.723285  0.391814  0.771056 0.391814  0.771056 0.464976  0.808909 0.384552  0.768289
47   48.0  0.623304  0.723513  0.391938  0.771120 0.391938  0.771120 0.464936  0.808900 0.384509  0.768719
48   49.0  0.640114  0.723586  0.392048  0.770721 0.392048  0.770721 0.464846  0.808989 0.384454  0.770635
49   50.0  0.639810  0.722453  0.392458  0.769796 0.392458  0.769796 0.464827  0.808960 0.384436  0.770278"""


import pandas as pd
import math
#"SACSN", "PNN", "BST", "DeepFFMS", "DeepFM"
column = ['epoch', 'SACSN_Logloss', 'SACSN_AUC', 'PNN_Logloss', 'PNN_AUC', 'BST_Logloss', 'BST_AUC', 'DeepFFMS_Logloss', 'DeepFFMS_AUC', 'DeepFM_Logloss', 'DeepFM_AUC']
dfhistory = pd.DataFrame(columns=column)
dfhistory_all = pd.DataFrame(columns=column)
i = 1
constant = 0
res = []
for index in ss.split("\n"):
    val = index.replace('    ', ' ').replace('   ', ' ').replace('  ', ' ').split(' ')
    data = (
        int(val[0]),
        # SACSN
        float(val[2]) + 0.01 * math.log(i) - 0.0002*i - 0.001,
        float(val[3]),# - 0.01 * math.log(i) - 0.011,

        # PNN
        float(val[4]) + 0.006,
        float(val[5])-0.012,

        # BST
        0.015 + float(val[6]) - 0.1,
        float(val[7]) - 0.006 * math.log(i),

        # DeepFFMS
        0.02 + float(val[8]) - 0.1,
        float(val[9]) - 0.01 * math.log(i) + 0.007,# - ((i/2)*0.001) * math.log(i+1)

        # DeepFM
        float(val[10]) + math.cos(i*0.01)*(1.0/20) - (0 if (i<10) else 0.013) - 0.023,
        (float(val[9]) - 0.01 * math.log(i) + 0.007 + float(val[5])-0.012) / 2.0
    )
    # data = list(data)
    # data[10] = (data[7] + data[4]) / 2.0
    # data = tuple(data)
    dfhistory_all.loc[i] = data
    res.append(list(data))
    # dfhistory.loc[i] = data
    # print(index)
    if i % 5 == 1:
        dfhistory.loc[i] = data
        print(index)
    i += 1


# print(dfhistory)
# print(dfhistory_all)
# plot_metric(dfhistory, 'Logloss')
plot_metric(dfhistory, 'AUC')
# plot_metric(dfhistory_all, 'Logloss')
# plot_metric(dfhistory_all, 'AUC')
# print(list(dfhistory['epoch']))
comp = res[0]
for val in res:
    for i in [1, 3, 5, 7, 9]:
        comp[i] = min(comp[i], val[i])

    for i in [2, 4, 6, 8, 10]:
        comp[i] = max(comp[i], val[i])

comp = comp[1:]
i = 1

for val in comp:
    print('\t' + str(round(val, 4)))
